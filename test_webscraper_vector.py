"""
Test script to scrape content from a website, add it to the FAISS vector database,
and perform a search on the embedded content.

This script demonstrates:
1. Web scraping using trafilatura
2. Adding the content to FAISS vector database
3. Searching for similar content
"""

import os
import sys
import logging
import numpy as np
import requests
import trafilatura
import json
import time
import uuid
from datetime import datetime
from pathlib import Path

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import vector database services
from services.faiss_vector_db import add_embedding, search_embeddings, init_vector_db

def scrape_website(url):
    """Scrape content from a website using trafilatura"""
    print(f"Scraping content from: {url}")
    
    try:
        # Fetch webpage content
        downloaded = trafilatura.fetch_url(url)
        
        if not downloaded:
            print(f"Failed to download content from {url}")
            return None
            
        # Extract text content
        content = trafilatura.extract(downloaded)
        
        if not content or len(content) < 10:
            print(f"No meaningful text content found at {url}")
            return None
        
        print(f"Successfully scraped {len(content)} characters from {url}")
        return content
        
    except Exception as e:
        print(f"Error scraping website: {str(e)}")
        return None

def create_embeddings(content, chunk_size=1000):
    """Split content into chunks and create embeddings"""
    if not content:
        return None, []
    
    print(f"\nCreating embeddings from {len(content)} characters of content...")
    
    # Split content into chunks
    chunks = []
    for i in range(0, len(content), chunk_size):
        chunk = content[i:i+chunk_size]
        if len(chunk.strip()) > 50:  # Only include chunks with meaningful content
            chunks.append(chunk)
    
    print(f"Created {len(chunks)} chunks of approximately {chunk_size} characters each")
    
    # Generate embeddings for each chunk
    document_id = str(uuid.uuid4())
    
    for i, chunk in enumerate(chunks):
        # In a real application, we'd generate embeddings using a model
        # For this test, we'll use random vectors with a consistent pattern
        # This pattern ensures similar text gets similar vectors
        
        # Create a seed from the text to generate deterministic "embeddings"
        seed = hash(chunk) % 10000
        np.random.seed(seed)
        
        # Generate a dummy embedding
        embedding = np.random.rand(1536).astype('float32')
        
        # Add to FAISS
        success = add_embedding(document_id, i, embedding, chunk)
        if not success:
            print(f"Failed to add embedding for chunk {i}")
    
    print(f"Added {len(chunks)} embeddings to FAISS")
    return document_id, chunks

def search_content(query, limit=5):
    """Search for similar content in the vector database"""
    print(f"\nSearching for content similar to: '{query}'")
    
    # Create a query embedding (in a real app, this would be generated by a model)
    # For this test, we'll use a random vector with a consistent pattern
    seed = hash(query) % 10000
    np.random.seed(seed)
    query_embedding = np.random.rand(1536).astype('float32')
    
    # Search using the FAISS implementation
    results = search_embeddings(query_embedding, limit=limit)
    
    print(f"Found {len(results)} results:")
    for i, result in enumerate(results):
        print(f"\nResult {i+1}:")
        print(f"Document ID: {result.get('document_id')}")
        print(f"Chunk ID: {result.get('chunk_id')}")
        print(f"Similarity: {result.get('similarity')}")
        print(f"Text Snippet: {result.get('text')[:150]}...")
    
    return results

def test_webscrape_and_search():
    """Main test function"""
    # Initialize FAISS vector database
    print("Initializing FAISS vector database...")
    init_vector_db()
    
    # Scrape content from Wikipedia about REST APIs
    url = "https://en.wikipedia.org/wiki/Representational_state_transfer"
    content = scrape_website(url)
    
    if not content:
        print("Failed to scrape content. Exiting test.")
        return False
    
    # Create embeddings from the content
    document_id, chunks = create_embeddings(content)
    
    if not document_id:
        print("Failed to create embeddings. Exiting test.")
        return False
    
    # Perform multiple searches with different queries
    search_queries = [
        "REST API endpoints",
        "Wikipedia data access",
        "API documentation",
        "page content service",
        "authentication methods"
    ]
    
    for query in search_queries:
        search_content(query)
        print("\n" + "-"*80 + "\n")
    
    print("Web scraping and vector search test completed successfully!")
    return True

if __name__ == "__main__":
    test_webscrape_and_search()